## 1. Параметрическое обучение в задачах распознавания
Параметрическое обучение основывается на параметрическом оценивании.
Параметрическое оценивание применяется в ситуациях, когда нам **известен аналитический вид** плотностей распределения вероятностей классов (например известно что данные распределены по гауссовскому закону), но **неизвестны параметры** этого распределения (например мат ожидание или матрица ковариации)

Задача обучения сводится к поиску оценки параметров $\hat \theta$ на основе обучающей выборки $X^N$ . В задачах  распознавания это позволяет восстановить функции правдоподобия для каждого класса $w_i$.
## 2. Свойства статистических оценок
Когда мы находим оценку $\hat \theta$, мы хотим чтобы она была максимально близка к истинному параметру $\theta$. Для этого она должна обладать следующими свойствами:
- **Несмещённость**: оценка называется несмещённой если её мат ожидание равно истинному значению параметра
$$M[\tilde\theta(X^N)] = M[\theta]$$
- **Состоятельность**: оценка называется состоятельной если при увеличении объёма выборки она сходится к истинному параметру  
  $$\lim_{N\rightarrow\infty} P[||\tilde\theta(X^N)-\theta|| < \epsilon], \epsilon > 0 $$
- **Эффективность**: оценка называется эффективной, если она имеет минимальную возможную дисперсию среди всех несмещённых оценок
  $$M[||\tilde\theta(X^N)-\theta||^2] \leq M[||\hat\theta(X^N)-\theta||^2]$$
- **Робастность** (Устойчивость): свойство оценки сохранять точность даже при нарушении исходных предположений о распределении или при наличии выбросов
## 3. Метод максимального правдоподобия
Суть метода: Мы выбираем такое значение параметров $\theta$, при котором наблюдаемая выборка становится наиболее вероятной. 

Для независимых и одинаково распределённых величин функция правдоподобия выглядит как произведение плотностей:
$$L(\theta) = P(X^N|\theta)=\prod_{i=1}^{N}p(x^{(k)}|\theta)$$
Переход к логарифму: работать с произведением сложно, особенно при дифференцировании. Поскольку логарифм - монотонно возрастающая функция, точка максимума $L(\theta)$ Совпадает с точкой максимума $ln(L(\theta))$
$$\ln L(\theta) = \sum_{k=1}^N\ln p(x^{(k)}|\theta)$$
## 4. Пример: оценка мат ожидания нормального распределения

Пусть $x~N(m,\sigma^2)$, где $\sigma^2$ известно, а $m$ - нет
1. Запишем формулу нормального распределения
   $$p(x^{(k)}|m, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}\cdot e^{\frac{(x^{(k)}-m)^2}{2\sigma^2}}$$
2. Запишем общую функцию правдоподобия
    $$L(m)=\prod_{k=1}^N\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{(x^{(k)}-m)^2}{2\sigma^2}}$$
3. Раскроем произведение
   $$L(m)=\left ( \frac{1}{\sigma\sqrt{2\pi}} \right)^N \cdot e^{-\sum_{k=1}^N\frac{(x^{(k)}-m)^2}{2\sigma^2}}$$
4. Запишем логарифм правдоподобия
   $$\ln L(m) =\ln\left ( \frac{1}{\sigma\sqrt{2\pi}} \right)^N - \sum_{k=1}^N\frac{(x^{(k)}-m)^2}{2\sigma^2}$$
5. Берём производную по $m$ и приравниваем к нулю
   $$\frac{d}{dm}\ln L(M)=\frac{1}{\sigma^2}\sum_{k=1}^N(x^{(k)}-m)=0$$
6. Решая уравнение, получаем оценку 
   $$\hat m = \frac{1}{N}\sum x^{(k)}$$

## 5. Подстановочные алгоритмы
Это решающие правила,  в которых теоретические параметры заменены оценками, полученными на этапе обучения.
В общем виде правило классификации выглядит так:
$$x \in \omega_1, \text{если }p(x|\omega_1, \theta_1)P(\omega_1) > p(x|\omega_2, \theta_2)P(\omega_2)$$
В подстановочном алгоритме мы просто используем оценки $\hat \theta$, найденные методом максимального правдоподобия:
$$x \in \omega_1, \text{если }p(x|\omega_1, \hat\theta_1)P(\omega_1) > p(x|\omega_2, \hat\theta_2)P(\omega_2)$$
Такой подход игнорирует неопределённость самой оценки, мы верим её как будто это истинный параметр. При малых выборках это может приводить к ошибкам.