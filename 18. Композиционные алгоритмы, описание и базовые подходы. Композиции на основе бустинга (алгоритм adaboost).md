
![[17. Композиционные алгоритмы, описание и базовые подходы#Композиционные алгоритмы]]

![[17. Композиционные алгоритмы, описание и базовые подходы#Бэггинг]]

![[17. Композиционные алгоритмы, описание и базовые подходы#Бустинг]]

# Особенноости композиций формируемых на основе бустинга:

-  последовательное обучение за несколько итераций -> *каждый следующий шаг опирается на предыдущий*
-  адаптация ошибок -> *увеличение весов объектов, классифицированных неправильно*
-  взвешенное объединение -> *итоговое решение - взвешенная сумма, вес каждого классификатора зависит от его точности*
**Цель - усиление слабых классификаторов.**

# Алгоритм Adaboost:

Классы образов $\omega_{1},\omega_{2}$ связаны с смешанной обучающей выборкой: $XD^N=\{ (x^{(i)},d^{(i)}), i = \overline{1,N} \}, \ d^{i}\in\{-1,+1\}$, 
БА:  $g_k(x) = \begin{cases} +1, & x \in \omega_1 \\ -1, & x \in \omega_2\end{cases}$, Композиция БА: $G(x)=sign[\sum^L_{k=1} \alpha_k g_k(x)]$
$\forall$ БА вводится *взвешенное число допущенных ошибок* - сумма весов всех объектов, на которых алгоритм ошибся:  

$$E(g_k,w)=\sum^N_{i=1} w_i I(g_k(x^{(i)})\ne d^{(i)},\  \sum^N_{i=1} w_i=1$$

**Отступ** - минимизируемый показатель качества распознавания: 

$$Q(g,\alpha)=\sum^N_{i=1}[M(x^{(i)})<0]= \sum^N_{i=1}[d^{(i)}\sum^L_{k=1} \alpha_k g_k(x^{(i)})<0] \rightarrow min $$

1. Одинаковые начальные веса $w_i=1/N, i=\overline{1,N}$ ($\Leftrightarrow$ априорным вероятностям)
2. $\forall$ шага $k=1,2...L$
	1) обучение БА $g_k(x)$ на $XD^N$, минимизирующего  $E_k=min\ E(g_k,w)$ ->(минимизация происходит за счет изменения порого при котором точка переходит в другой класс)
	2) если $E_k=0$ то $G(x)=g_k(x)$ [п.3]
	3) если $E_k\geq 1/2$ то [п.4] -> критическая ошибка
	4) зафиксировать $\alpha_k=\frac {1}{2} ln(\frac {1-E_k}{E_k})$ 
	5) пересчет весов  $w^{'}_i=w_i exp(-\alpha_k d^{(i)} g_k(x^{(i)})), i=\overline{1,N}$
	6) нормировка весов $\sum^N_{i=1} w^{'}_i=1$
3. Зафиксировать композицию  $G(x)=sign[\sum^L_{k=1} \alpha_k g_k(x)]$
4. Конец

> [!note] При перевзвешивании усиливается роль ошибочно классифицированных объектов, учитываются ранее допущенные ошибки**

$\Leftrightarrow$
замена отступов на $[M(x^{(i)})<0]\leq exp[-M(x^{(i)})]$
минимизация функции

$$\overline Q(g,X)=\sum^N_{i=1}exp[-M(x^{(i)})]\geq Q(g,X), H(M)=exp(-M)$$

В качестве БА чаще всего используются **деревья решений малой высоты** и простые правила порогового типа.
