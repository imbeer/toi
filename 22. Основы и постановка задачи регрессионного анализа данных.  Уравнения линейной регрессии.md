# Общие сведения

Задача регрессии является одной из задач анализа данных и относится к классу предсказательных (predictive) задач. Предсказательные задачи осуществляются в два этапа:

1. Построение модели анализируемого объекта (процесса) путем обработки данных – получаемых в ходе экспериментов наблюдений, в той или иной степени, его характеризующих.
2. Модель используется для предсказания результатов по отношению к новым наборам данных.

При решении задачи регрессии модель объекта строится в виде функциональной зависимости набора непрерывных выходных переменных от заданного набора входных, а второй этап направлен на определение значений выходных переменных при появлении новых значений входных.

# Постановка задачи

Задача регрессии состоит в в установлении функциональной зависимости между зависимыми и независимыми показателями и переменными в понятной форме.

Обычно вектор входных (независимых) переменных (регрессоров, предикторов) $x \in X \subseteq \mathbb{R}^n$, а вектор выходных (зависимых) переменных – $y \in Y \subseteq \mathbb{R}^1$ (скаляр). В случае, если $n = 1$, говорят о постановке **задачи парной регрессии,** а если $n > 1$, то о **множественной регрессии.** 

В более широкой постановке может рассматриваться несколько зависимых переменных, которые объединяются в вектор $y \in \mathbb{R}^h$.

Если значение зависимой переменной определяется по отношению к будущему моменту времени, то задача регрессии называется **задачей прогнозирования (экстраполяции)**.

# Линейная регрессия

## Метод наименьших квадратов (МНК)

Дана обучающая выборка данных $X^N = \left\{ x^{(1)}, \ldots, x^{(N)} \right\}, x^{(i)} \in X \subseteq \mathbb{R}^n$, в которой каждому значению вектора-столбца входных переменных соответствует совокупность значений выходной переменной $Y^N = \left\{ y^{(1)}, \ldots, y^{(N)} \right\}, y^{(i)} \in Y \subseteq \mathbb{R}^1$.

Требуется построить отображение вида $\tilde \varphi: X \rightarrow Y$. Для аппроксимации используются функции вида $\psi(x,a), a \in \mathbb{R}^m$, где $a$ – вектор неопределённых параметров модели.

В рамках МНК требуется найти значение $\tilde a$, для которого минимизируется функционал

$$J(a) = \sum_{i=1}^N \left( \psi(x^{(i)},a) - y^{(i)} \right)^2 \rightarrow \min_a. \tag{1}$$

Как необходимое условие минимума выполняется

$$\left.\frac {\partial J} {\partial a}\right|_{a=\tilde a} = 2 \sum_{i=1}^N \left( \psi(x^{(i)},a) - y^{(i)} \right) \frac {\partial \psi(x^{(i)},a)} {\partial a} = 0. \tag{2}$$

## Линейная параметрическая регрессия

В качестве $\psi(x,a)$ используется линейная функция

$$\psi(x,a) = a_0 + \sum_{k=1}^n a_k x_k,$$

где $a_0$ – скалярная величина; $a_k = (a_1, \ldots, a_n)^T$ – вектор коэффициентов при компонентах входного вектора размера $n \times 1$.

Добавим фиктивную компоненту $x_0 = 1, $x_e = (x_0, x_1, \ldots, x_n)^T$$ и объединим коэффициенты в общий вектор $a_k = (a_0, a_1, \ldots, a_n)^T$, тогда

$$\psi(x,a) = a^T x_e = \sum_{k=0}^n a_k x_k.$$

Введём матрицу $X$ размера $N \times (n+1)$, строками которой являются транспонированные расширенные векторы входной обучающей выборки, и вектор выходных данных выборки

$$X = \begin{pmatrix} x_e^{(1),T} \\ x_e^{(2),T} \\ \vdots \\ x_e^{(N),T} \end{pmatrix} = \begin{pmatrix} 1 & x_1^{(1)} & \cdots & x_n^{(1)} \\ 1 & x_1^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_1^{(N)} & \cdots & x_n^{(N)} \end{pmatrix}, y = \left( y^{(1)}, \ldots, y^{(N)} \right)^T = \begin{pmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{pmatrix}.$$

Из общего решения (2) получим следующую систему $n+1$ уравнений в матричном виде, обеспечивающих минимизацию невязки $Xa-y$:

$$J(a) = ||Xa-y||^2 = (Xa-y)^T(Xa-y) \rightarrow \min_a,$$

$$\left.\frac {\partial J} {\partial a}\right|_{a=\tilde a} = 2X^T(Xa-y) = 2 \sum_{i=1}^N \left( a^T x_e^{(i)} - y^{(i)} \right)x_e^{(i)} = 0, \frac {\partial \psi(x^{(i)},a)} {\partial a} = \frac {\partial a^T x_e^{(i)}} {\partial a} = x_e^{(i)}.$$

Тогда исходная система уравнений для нахождения $\tilde a$ перепишется в виде другой системы, называемой **нормальной системой,** которая в матричной форме имеет следующий вид:

$$X^Ty-X^T\tilde a=0.$$

Нормальная система *всегда совместна.* Матрица $H=X^T X$ является квадратной матрицей размера $(n+1)$. В случае, если $H$ невырожденная, существует единственное, так называемое *нормальное* решение

$$\tilde a = (X^T X)^{-1} X^T y = X^+y, X^+ = (X^T X)^{-1} X^T.$$

Здесь матрица $X^+$ – псевдообратная для $X$, она обладает следующими свойствами, сходными со свойствами обратной матрицы:

$$X^+X=I,\quad X^+XX^+=X^+,\quad XX^+X=X.$$

Если $X$ квадратная и невырожденная, то полученное решение является решением СЛАУ $Xa=y$ по правилу Крамера: $\tilde a = X^{-1}y$, при этом невязка тождественно равна нулю.

Однако на практике часто СЛАУ $Xa=y$ переопределена, тогда окончательное решение задачи имеет вид

$$\tilde y = \psi(x, \tilde a) = \tilde a^Tx = \tilde a_0 + \sum_{k=1}^n \tilde a_i x_i.$$

Величины $e^{(i)} = y^{(i)} - \tilde y^{(i)}, i = \overline{1,N}, \tilde y^{(i)} = \tilde a^T x^{(i)}$ называются *остатками.*