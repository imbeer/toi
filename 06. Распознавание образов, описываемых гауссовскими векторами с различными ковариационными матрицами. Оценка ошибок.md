## 1. Квадратичное решающее правило

Когда матрицы ковариации классов различны ($C_1 \neq C_2$), логарифм отношения правдоподобия (ЛОП) становится **квадратичной функцией** вектора признаков $x$. В отличие от случая с одинаковыми матрицами ковариации, разделяющая граница здесь уже не является гиперплоскостью.

Решающее правило для двух классов имеет вид:
$$g'(x) = \;\overset{\omega_1}{\underset{\omega_2}{\gtrless}}\; l_0'$$

Преобразуем выражение для ЛОП $g'(x)$:

$$
\begin{aligned}
g'(x) =\;&
-\frac{1}{2}\ln|C_1|
-\frac{1}{2} x^{T} C_1^{-1} x
+\frac{1}{2} m_1^{T} C_1^{-1} x
+\frac{1}{2} x^{T} C_1^{-1} m_1
-\frac{1}{2} m_1^{T} C_1^{-1} m_1 \\
&+ \frac{1}{2}\ln|C_2|
+ \frac{1}{2} x^{T} C_2^{-1} x
- \frac{1}{2} m_2^{T} C_2^{-1} x
- \frac{1}{2} x^{T} C_2^{-1} m_2
+ \frac{1}{2} m_2^{T} C_2^{-1} m_2 \\
= \;&
-\frac{1}{2} x^{T} (C_1^{-1} - C_2^{-1}) x
+ x^{T} (C_1^{-1} m_1 - C_2^{-1} m_2) \\
&- \frac{1}{2} m_1^{T} C_1^{-1} m_1
+ \frac{1}{2} m_2^{T} C_2^{-1} m_2
- \frac{1}{2} \ln \frac{|C_1|}{|C_2|}
\end{aligned}
$$

В данном случае зависимость разделяющей функции $g'(x) = g''(x) - l'$ от компонент вектора признаков носит **квадратичный характер**. Первое слагаемое содержит квадратичную форму $x^T (C_1^{-1} - C_2^{-1}) x$, что принципиально отличает этот случай от линейного.

## 2. Геометрические особенности разделяющих границ

Различные комбинации математических ожиданий и матриц ковариации приводят к различным формам разделяющих границ:

- **а)** Математические ожидания и матрицы ковариаций разные: граница имеет вид **параболы**. Для класса слева коэффициент корреляции положительный, для класса справа — отрицательный.

- **б)** Математические ожидания одинаковые, дисперсии разные: граница представляет собой **окружность**, вложенную в область локализации первого класса.

- **в)** Математические ожидания одинаковые, матрицы ковариаций разные: граница имеет вид **ломанных прямых линий**.

- **г)** Математические ожидания и матрицы ковариаций разные: граница определяется **гиперболами**, при этом область второго класса может быть разрывной (например, область внутри "песочных часов" относится к первому классу, остальное — ко второму).

## 3. Оценка вероятностей ошибок

В отличие от случая с одинаковыми матрицами ковариации, где ЛОП распределено по гауссовскому закону, здесь распределение $g'(x)$ не является гауссовским. Поэтому для оценки вероятностей ошибок используются различные приближенные методы.

### 3.1. Гауссовская аппроксимация плотностей

При достаточных значениях размерности пространства признаков $n$ гауссовская аппроксимация позволяет получить приближенные, но во многих случаях приемлемые результаты. При этом математические ожидания и дисперсии $g'(x)$ для обеих гипотез имеют следующий вид:

**Для гипотезы $\omega_1$:**

$$
m_{g_1} = M[g' / \omega_1]
= \frac{1}{2}\,\operatorname{tr}(C_2^{-1}C_1 - I)
+ \frac{1}{2}(m_1 - m_2)^T C_1^{-1}(m_1 - m_2)
- \frac{1}{2}\ln\frac{|C_1|}{|C_2|}
= h_1
$$

$$
D_{g_1} = M[(g' - m_{g_1})^2 / \omega_1]
= \frac{1}{2}\operatorname{tr}\big[(C_2^{-1}C_1 - I)^2\big]
+ (m_1 - m_2)^T C_1^{-1} C_2 C_1^{-1}(m_1 - m_2)
= d_1
$$

**Для гипотезы $\omega_2$:**

$$
m_{g_2} = M[g' / \omega_2]
= \frac{1}{2}\operatorname{tr}(I - C_1^{-1}C_2)
- \frac{1}{2}(m_1 - m_2)^T C_2^{-1}(m_1 - m_2)
+ \frac{1}{2}\ln\frac{|C_2|}{|C_1|}
= h_2
$$

$$
D_{g_2} = M[(g' - m_{g_2})^2 / \omega_2]
= \frac{1}{2}\operatorname{tr}\big[(I - C_1^{-1}C_2)^2\big]
+ (m_1 - m_2)^T C_1^{-1} C_2 C_1^{-1}(m_1 - m_2)
= d_2
$$

Обратите внимание, что в отличие от случая с одинаковыми матрицами ковариации, здесь математические ожидания и дисперсии **не симметричны** и **не равны** друг другу.

Используя гауссовскую аппроксимацию:
$$p(g' / \omega_1) \approx \mathcal{N}(g', m_{g_1}, D_{g_1}), \qquad p(g' / \omega_2) \approx \mathcal{N}(g', m_{g_2}, D_{g_2})$$

вероятности ошибок первого и второго рода вычисляются как:

$$\alpha = \int_{-\infty}^{l'_0} \mathcal{N}(g', m_{g_1}, D_{g_1})\,dg' = \Phi\!\left(\frac{l'_0 - m_{g_1}}{\sqrt{D_{g_1}}}\right)$$

$$\beta = \int_{l'_0}^{\infty} \mathcal{N}(g', m_{g_2}, D_{g_2})\,dg' = 1-\Phi\!\left(\frac{l'_0 - m_{g_2}}{\sqrt{D_{g_2}}}\right)$$

где $\Phi(\cdot)$ — функция стандартного нормального распределения.

### 3.2. Верхние границы Чернова с расстоянием Бхаттачария

Для получения верхних оценок вероятностей ошибок используется метод границ Чернова с расстоянием Бхаттачария:

$$
\begin{aligned}
\alpha &\leq \sqrt{p(\omega_2)/p(\omega_1)} \exp\left(-\mu\left(\frac{1}{2}\right)\right), \\
\beta &\leq \sqrt{p(\omega_1)/p(\omega_2)} \exp\left(-\mu\left(\frac{1}{2}\right)\right), \\
E_s &= p(\omega_1)\alpha + p(\omega_2)\beta \leq \sqrt{p(\omega_1)p(\omega_2)} \exp\left(-\mu\left(\frac{1}{2}\right)\right), \\
\mu\left(\frac{1}{2}\right) &= \frac{1}{8}(m_1 - m_2)^T \frac{(C_1 + C_2)^{-1}}{2}(m_1 - m_2) + \frac{1}{2} \ln \frac{1}{2} \frac{|C_1| + |C_2|}{\sqrt{|C_1||C_2|}}.
\end{aligned}
$$

Здесь $\mu(1/2)$ — расстояние Бхаттачария между распределениями классов, которое учитывает как различие математических ожиданий, так и различие ковариационных матриц.

### 3.3. Метод статистического моделирования

При достаточных объемах данных метод статистического моделирования (метод Монте-Карло) дает наиболее точные результаты. Он основан на генерации большого числа реализаций случайных векторов из каждого класса и подсчете частоты ошибок классификации.

## 4. Многоклассовое распознавание

Когда необходимо распределить объекты по $M$ классам с различными матрицами ковариации ($C_i \neq C_j$ при $i \neq j$), разделяющие функции $g_i'(x)$ становятся **квадратичными** относительно признаков $x$.

Правило решения остается прежним: объект относится к классу $\omega_i$, если его разделяющая функция в этой точке максимальна:
$$\omega_i: g_i'(x) \ge g_j'(x) \text{ для всех } j \ne i$$

Разделяющие функции принимают вид:
$$g_i'(x) = -\frac{1}{2} x^T C_i^{-1} x + x^T C_i^{-1} m_i - \frac{1}{2} m_i^T C_i^{-1} m_i - \frac{1}{2} \ln|C_i| + \ln(p(\omega_i))$$

Границы между областями в многомерном пространстве представляют собой фрагменты **квадрик** (квадратичных поверхностей). Каждая такая граница — это результат попарного сравнения двух классов и может иметь различные геометрические формы в зависимости от соотношения их параметров.

## 5. Оценка вероятности ошибок для многоклассового распознавания

Для многоклассового случая с различными матрицами ковариации рассчитать точную вероятность ошибки еще сложнее, чем для двух классов. Используются те же подходы, что и для двух классов:

- **Нижняя граница** через вероятности попарных ошибок: $P_c^{(i)} \ge 1 - \sum_{j=1, j \ne i}^{M} p_{ij}$, где $p_{ij}$ — вероятность того, что объект класса $i$ будет принят за класс $j$ при их попарном сравнении.

- **Верхние границы** через суммы попарных границ Чернова.

- **Метод статистического моделирования** для получения наиболее точных оценок при наличии достаточного объема данных.
