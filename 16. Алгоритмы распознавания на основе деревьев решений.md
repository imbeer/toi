
**Идея, лежащая в основе деревьей решений:** 
1. разбиение множества всех возможных значений признаков на непересекающиеся множества;
2. настройка модели решения для каждого такого подмножества.

Дерево решений - метод представления решающих правил в дереве (правила находятся в узлах, каждый узел разбивает по одному признаку).

# Алгоритм формирования дерева решений

Пусть $XD^N$ - обучающая выборка, $X \subseteq R$ - пространство образов.

Для каждой вершины $t$:
1. задано помножество $X_t \subset X$, при этом с корневой вершиной связано все $X$.
2. задана подвыборка обучающей выборки $XD_t \subset XD^N$, при этом $x \in X_t$, с корневой вершиной связано все $XD^N$
3. задано правило $h_t: X_t \rightarrow \lbrace 0, 1,...,k_{t-1} \rbrace$, где $k_t \ge 2$ - количество потомков вершины $t$ (для листьев не вводится).

Пусть $t_{i(t)}$, $i = 0,1,...,k_{t-1}$ вершина, являющайся $i$-м потомком вершины $t$. 
Тогда ее обучающая выборка: $X_{i(t)} = X_t \cap \lbrace x \in X: h_t(x) = i \rbrace$. 

# Цель построения деревьев

заключается в том, что мы применяем все правила по очереди и находим терминальную вершину (лист дерева). 
$x$ для которого подобрали лист дерева относят к наиболее популярному классу в подвыборке этого листа.


# CART - Classification and Regression Trees.

Рекурсивно разбивает обучающие выборки на две более однородные подвыборки по одному из признаков. Техники:

1. Показатель загрязненности - impurity
   $J(t)$ - загрязненность вершины. $J(t) = 0$, если в вершине только один класс, $J(t) \rightarrow \max$, если в вершине много классов. 
	Есть много методов расчета таких показателей, один из них - **метод Джини:**
	
	$$J(t) = 1 - \sum^M_{i=1} N^2_t(\omega_i)$$


2. Расщепление деревьев - split
	Правило разбиения множества $X$ для каждой вершины называется расщеплением. Бинарное расщепление это функция:  $h_t: X_T \rightarrow \lbrace 0,1 \rbrace, x\in X_t$
	Расщепление должно минимизировать показатель загрязненности. При этом области решения будут представлять собой многомерные параллелепипеды, каждая граница - гиперплоскость, параллельная одной из координатных осей пространства признаков.


3. Критерий остановки расщепления
	Полное дерево обычно обладает низкой достоверностью классификации из-за переобучения, поэтому нужно вводить критерий остановки расщепления (например минимальное число для количества наблюдений в подвыборке).

4.  Усечение деревьев - pruning
	Вместо того чтобы останавливать расщепление по какому-то критерию мы строим полные деревья, а затем усекаем (заменяем целое вершину и поддерево терминальной вершиной).