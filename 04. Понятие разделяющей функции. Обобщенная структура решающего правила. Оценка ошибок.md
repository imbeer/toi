
# Понятие разделяющей функции

Исходя из результатов синтеза алгоритмов, обобщенная структура байесовского классификатора определяется канонической формой представления с использованием разделяющих функций.

**Разделяющие функции** — набор функций $g_i(x), i = \overline{1,M}$, на основе которых определяется соответствие вектора признаков одному из классов по правилу:

$$\omega_i : g_i(x) \ge g_j(x), \quad j = \overline{1, M}, \quad i \neq j,$$

т.е. отнесение образа к классу на основе максимума среди значений разделяющих функций.
![[Pasted image 20260115235651.png]]

# Обобщенная структура решающего правила

Решение формируется как индекс класса с максимальным значением разделяющей функции. Для трех вариантов алгоритмов ([[02. Байесовская теория решений. Решающее правило на основе минимума условного риска#Критерий минимума условного риска (МУР)|МУР]], [[03. Байесовская теория решений. Решающие правила на основе максимума апостериорной вероятности и функции правдоподобия#Критерий максимума апостериорной вероятности (МАВ)|МАВ]], [[03. Байесовская теория решений. Решающие правила на основе максимума апостериорной вероятности и функции правдоподобия#Критерий максимального правдоподобия (МП)|МП]]) разделяющие функции:

$$g_i(x) = \begin{cases} -r(\alpha_i / x), & \text{для МУР} \\ p(\omega_i / x); p(\omega_i) p(x / \omega_i), & \text{для МАВ} \\ p(x / \omega_i), & \text{для МП} \end{cases} \quad i = \overline{1,M}.$$

Если $g_i(x), i = \overline{1,M}$ — разделяющие функции, то любая монотонно возрастающая функция $g'(x) = f(g(x))$ дает эквивалентное правило для $g_i'(x), i = \overline{1,M}$.

Удобно использовать натуральный логарифм: $g_i'(x) = \ln(g_i(x)), i = \overline{1,M}$, — произведения и степени преобразуются в суммы.

**Для двух классов обобщенное правило:**

$$g_1(x) \underset{\omega_2}{\overset{\omega_1}{\gtrless}} g_2(x), \quad \Leftrightarrow \quad g(x) = g_1(x) - g_2(x) \underset{\omega_2}{\overset{\omega_1}{\gtrless}} 0.$$

Классификатор двух классов — алгоритм, вычисляющий единственную разделяющую функцию. Граница между $\Gamma_1, \Gamma_2$ — уравнение $g_1(x) - g_2(x) = 0$.

Для МАВ разделяющая функция:

$$g(x) = p(\omega_1/x) - p(\omega_2/x)$$

или эквивалентно:

$$g'(x) = \ln p(\omega_1/x) - \ln p(\omega_2/x) = \ln \frac{p(x/\omega_1)}{p(x/\omega_2)} + \ln \frac{p(\omega_1)}{p(\omega_2)}.$$

Для общей формы двух гипотез:

$$l(x) = \frac{p(x/\omega_1)}{p(x/\omega_2)} \underset{\omega_2}{\overset{\omega_1}{\gtrless}} = l_0,$$

где $l_0$– порог, задаваемый для каждого из алгоритмов МУР, МАВ, МП, логарифмирование выполняется и для порога. Т.е. общий вид алгоритма классификации и уравнение для границы классов в пространстве признаков будет иметь вид

$$g'(x) \underset{\omega_2}{\overset{\omega_1}{\gtrless}} l_0' , \quad g'(x) = \ln l(x) = \ln \frac{p(x/\omega_1)}{p(x/\omega_2)}, \quad l_0' = \ln l_0, \quad g'(x) - l_0' = 0.$$

## Оценка вероятностей ошибок распознавания

Анализ синтезированных алгоритмов включает оценку ожидаемых потерь и ошибок, вид разделяющих функций (линейные/нелинейные), формы областей $\Gamma ^{(i)}, i = \overline{1,M}$.

Базовые показатели: вероятности правильного распознавания $P_c^{(i)}$ и ошибки $\varepsilon_{er}^{(i)},  i = \overline{1,M}$:

$$P_c^{(i)} = 1 - \varepsilon_{er}^{(i)} = \int_{\Gamma_i} p(x / \omega_i) dx, \quad \varepsilon_{er}^{(i)} =  \int_{\overline\Gamma_j} p(x / \omega_i) dx, \quad i = \overline{1, M}. \quad (7)$$

Общая суммарная ошибка:

$$E_s = \sum_{i=1}^{M} p(\omega_i) \varepsilon_{er}^{(i)} = 1 - \sum_{i=1}^{M} p(\omega_i) P_c^{(i)}.$$

Матрица ошибок распознавания:

$$M_{er} = \|\varepsilon_{ij}\| , \quad \varepsilon_{ij} = \int_{\Gamma_j} p(x / \omega_i) dx, \quad \sum_{j=1, j \neq i}^{M} \varepsilon_{ij} = \varepsilon_{er}^{(i)}, \quad \varepsilon_{ii} = P_c^{(i)} = 1 - \sum_{j=1, j \neq i}^{M} \varepsilon_{ij}.$$

Для двух классов: ошибки первого ($\alpha$) и второго ($\beta$) рода:

$$\alpha = \int_{\Gamma_2} p(x / \omega_1) dx, \quad \beta = \int_{\Gamma_1} p(x / \omega_2) dx. \quad (8)$$

![[Pasted image 20260116005100.png]]
![[Pasted image 20260116005117.png]]

Для МУР вероятности ошибок пересчитываются в потери:

$$R = \sum_{i=1}^{M} \sum_{j=1}^{M} \lambda(\alpha_i / \omega_j) p(\omega_j) \int_{\Gamma_i} p(x / \omega_j) dx = \sum_{i=1}^{M} \sum_{j=1}^{M} \lambda(\alpha_i / \omega_j) \varepsilon_{ji} p(\omega_j).$$

Для симметричной функции потерь:

$$R = \sum_{i=1}^{M} \int_{\Gamma_i} [1 - p(\omega_i / x)] p(x) dx = 1 - \sum_{i=1}^{M} p(\omega_i) \int_{\Gamma_i} p(x / \omega_i) dx = 1 - \sum_{i=1}^{M} p(\omega_i) P_c^{(i)} = E_s.$$

МАВ минимизирует $E_s$ (критерий идеального наблюдателя).


Способы расчета ошибок:

1. **Первый способ:** Прямое интегрирование $p(x/\omega_i)$ по $\Gamma_i$ (затруднительно для $n>1$).

2. **Второй способ:** Анализ одномерных распределений разделяющих функций $p(g_i/\omega_j)$, с приближениями (гауссово для $p(g'/\omega_i)$, $g'(x)=\ln l(x)$). Для двух классов:

$$\alpha = \int_{-\infty}^{l_0'} p(g'/\omega_1)dg', \quad \beta = \int_{l_0'}^{\infty} p(g'/\omega_2)dg'.$$

3. **Третий способ:** Принцип гарантированного результата, верхние границы (граница Чернова). Для двух классов:
   
   $$E_s = p(\omega_1)\alpha + p(\omega_2)\beta \le p(\omega_1)^{1-t} p(\omega_2)^t \exp(-\mu(t)), \quad \mu(t) = -\ln \int p(x / \omega_1)^{1-t} p(x / \omega_2)^t dx,$$
   
   оптимальное $t$ – скалярная величина, лежащая в интервале от нуля до единицы:
   
   $$-\frac{d\mu(t)}{dt} = \ln \frac{p(\omega_1)}{p(\omega_2)}.$$
   
   $\mu(1/2)$ — расстояние Бхатачария.
   
   Для многих классов:
   
   $$E_s < \sum_{j=1}^{M} \sum_{i>j}^{M} E_{s_{ij}},$$
   
   где $E_{s_{ij}}$ — границы для попарного распознавания.

4. **Четвертый способ:** Статистическое имитационное моделирование: подсчет ошибок по $K$ реализациям. Доверительные интервалы: 
   
   $$P\left[|P_c^{(i)} - \tilde{P}_c^{(i)}| < d_g\right] = 1 - \gamma,$$
   
   где $d_g$ – доверительный интервал; $\gamma$ – уровень значимости. Конечное уравнение, связывающее $d_g$  и K, имеет вид
   
   $$d_g = t_{kp}(\gamma)\sqrt{P_c^{(i)}(1-P_c^{(i)})/K}, \quad K = \frac{t_{kp}^2(\gamma)P_c^{(i)}(1-P_c^{(i)})}{d_g^2}, \quad 2\Phi(t_{kp}(\gamma)) - 1 = 1 - \gamma,$$
   
   $$t_{kp}(\gamma) = \Phi^{-1}((2-\gamma)/2), \quad \Phi(t) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{t} e^{-\frac{x^2}{2}} dx.$$
   
   Для гарантии заменить $P_c^{(i)}(1-P_c^{(i)})$ на $0.25$:
   
   $$d_g \geq \frac{t_{kp}(\gamma)}{2\sqrt{K}}, \quad K \geq \frac{t_{kp}^2(\gamma)}{4d_g^2}.$$

Таблица 1. Рекомендуемые объемы статистических испытаний (для $\gamma = 0.05$, $t_{kp}=1.96$)

| Значения оцениваемой вероятности ошибки $(1 - P_c^{(i)})$ | Количество испытаний K $(d_g = 0.25(1 - P_c^{(i)}))$ | Количество испытаний K $(d_g = 0.1(1 - P_c^{(i)}))$ |
| --------------------------------------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| 0.1                                                       | 5.532e+02                                            | 3.457e+03                                           |
| 0.01                                                      | 6.085e+03                                            | 3.803e+04                                           |
| 0.001                                                     | 6.140e+04                                            | 3.837e+05                                           |

Использовать количество испытаний почти на два порядка превышающее величину, обратную оцениваемой малой вероятности ошибки