
# Непараметрическое обучение

**Задача непараметрического оценивания** — задача, в которой неизвестны параметры распределений и сами виды плотностей вероятностей или функций.

**Решение:** требуется найти приближённую функцию $p$~$(x)$, обладающую следующими свойствами:

1. **Сходимость по вероятности (состоятельность):**  
   
   $$ \lim_{N \to \infty} P(|p(x) - \tilde{p}(x)| < \varepsilon) = 1 $$
   
   Это означает, что с ростом объёма обучающей выборки вероятность отклонения оценки от истинного значения становится сколь угодно малой.
2. **Сходимость в среднеквадратичном (асимптотическая несмещённость):**  
   
   $$ \lim_{N \to \infty} M[\tilde{p}(x)] = p(x)\lim_{N \to \infty} M[|p(x) - \tilde{p}(x)|^2] = 0 $$
   
   
# Метод k-ближайших соседей

**Идея метода:** сделать объём ячеек переменным, т.е. зависящим от данных, а не фиксированным, так чтобы в ячейку попадало ровно $k$ элементов обучающей выборки.

## Оценка плотности распределения

Оценка плотности распределения в точке $x$ имеет вид:

$$ \tilde{p}(x) = \frac{k}{N \cdot V_n(k, N, X^N)} $$

где:

- $k$ — число ближайших соседей;
- $N$ — объём обучающей выборки;
- $V_n(k,N,X^N)$ — объём области, содержащей k ближайших соседей точки x;
- $r=r(x,X^N)$ — расстояние от точки x до k-го ближайшего соседа.

При определении близости может использоваться любая метрика.

## Евклидова метрика

При использовании евклидовой метрики объём гипершара равен:

$$ V_n = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)} r^n $$

где Γ(⋅) — гамма-функция.

## Выбор параметра kk

Рекомендуемые зависимости:

$$ k = a N^\gamma, \quad 0 < \gamma < 1 или k = b \ln(N + 1) $$  

где $a,b,\gamma$ — константы.

Если выполняются условия:  

$$ \lim_{N \to \infty} k = \infty, \quad \lim_{N \to \infty} \frac{k}{N} = 0 $$ 

то оценка является асимптотически несмещённой и состоятельной.

## Точность оценки

- Точность оценки невелика при небольших объёмах обучающей выборки N.
- При увеличении числа используемых соседей точность оценки повышается и становится сравнимой с оценками на основе метода Парзена.

## Решающее правило

В гипершар объёма $V_n$​ попадает $k_i​$ образов класса $\omega_i$​. Тогда:

$$ \tilde{p}(\omega_i \mid x) \sim \frac{k_i}{k} $$

**Решающее правило максимума апостериорной вероятности (МАВ):**

$$ \omega_j : k_j = \max_{i} k_i $$

Для двух классов:

$\omega_1,$ если $k_1>k_2$       $\omega2,$ если $k2>k1$.

## Границы суммарной вероятности ошибки

$$ E_s \le \tilde{E}_s \le 2 E_s $$

где $E_s$​ — вероятность суммарной ошибки оптимального байесовского решающего правила при известных функциях правдоподобия классов.

### Почему k-соседей, если Парзен лучше?

1. Метод Парзена требует выбора большого числа гиперпараметров (вид ядра, размер окна, центрирование), что особенно сложно в многомерных задачах.
2. Метод k-ближайших соседей имеет только один гиперпараметр — k, определяемый на основе расстояний, что делает метод более простым и практичным.