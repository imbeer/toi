### Композиционные алгоритмы, описание и базовые подходы.
**Композиционные алгоритмы** - совокупность результатов  работы нескольких классификаторов 
***Основная идея:*** ансабль дает более точный результат, чем каждый классификатор в отдельности
##### Общая структура:
%% основа: множество точек прогоняют через L классификаторов, каждый из которых на выходе дает оценку, объединяем эти оценки в одну и принимаем решение к какому классу отнести точку%%
Классы образов $\omega_{1},...,\omega_{M}$ связаны с смешанной обучающей выборкой: 
$$\begin{aligned}
X^N = \{ x^{(1)},...,x^{(N)} \},\ D^N = \{ d^{(1)},...,d^{(N)} \}, \ d^*\in D = \{1,...,M\}, \ x^*\in \omega_j, \ d^* = j,\\
\{ x^{(i)},d^{(i)}, i = \overline{1,N} \}
\end{aligned}$$
Вводится пространство оценок $R$, алгоритмы имеют вид суперпозиции $g^{'}(x) =g^{'}[g(x)]$ , где $g : X \rightarrow R$  *базовый алгоритм*, а $g^{'} : R \rightarrow D$ *решающее правило*.
**Композиция БА** $g_k(x),k=\overline{1,L}$  - это решающее правило:  $$G^{'}(x)=G^{'}[G(g_1(x),...g_L(x))], G:R^L \rightarrow R \ (корректирующая \ операция), G^{'}:R \rightarrow D$$**Замечание:** использование корректирующе опрации для получения комбинированной оуенки на R расширяет рассматриваемые варианты объединения БА.

**Линейный ансамбль БА**(композиция): $$G^{'}(x)=G^{'}[\sum^L_{k=1} \alpha_k g_k(x)],\  \sum^L_{k=1} \alpha_k=1$$где $\alpha_k \geq 0, k=\overline{1,L}$ - весовые коэффициенты БА.
Если $\alpha_k =1/L, k=\overline{1,L}$  - **простое голосование**

Для случая двух классов для принятия решений используется функция знака - $sign[g_k(x)]$. По вероятности ошибки $p_g$ определяют статус классификатора:
- $p_g \geq 1/2$  плохой
- $p_g = 1/2 + \epsilon$  слабый
- $p_g =\epsilon$   сильный
##### Базовые подходы КА: 
1. **Бэггинг** (bagging или bootstrap aggregating) – Из исходной выборки извлекается много случайных копий (бутстреп — случаное извлечение примеров, можем вытащить один и тот же дважды). На каждой такой копии обучаем отдельный классификатор.
   Все классификаторы равноправны(**простое голосование**).
   **Общая идея** – снижение зависимости БК ансамбля друг от друга%%(если ошибся один, на остальные это не повлияет)%%.
2. **Бустинг** (boosting – улучшение) - последовательное обучение классификаторов, увеличение весов неправильно классифицированных объектов(перевзвешивание наблюдений)
   Классификаторы не равноправны(**взвешенное голосование**)
   **Общая идея** – БК учатся на ошибках допущенных предыдущими классификаторами%%(Усиление слабых классификаторов)%%.

---
### Композиции на основе бустинга (алгоритм adaboost)
##### Особенноости композиций формируемых на основе бустинга:
-  последовательное обучение за несколько итераций -> *каждый следующий шаг опирается на предыдущий*
-  адаптация ошибок -> *увеличение весов объектов, классифицированных неправильно*
-  взвешенное объединение -> *итоговое решение - взвешенная сумма, вес каждого классификатора зависит от его точности*
**Цель - усиление слабых классификаторов.**

##### Алгоритм Adaboost:
Классы образов $\omega_{1},\omega_{2}$ связаны с смешанной обучающей выборкой: $XD^N=\{ (x^{(i)},d^{(i)}), i = \overline{1,N} \}, \ d^{i}\in\{-1,+1\}$, 
БА:  $g_k(x) = \begin{cases} +1, & x \in \omega_1 \\ -1, & x \in \omega_2\end{cases}$, Композиция БА: $G(x)=sign[\sum^L_{k=1} \alpha_k g_k(x)]$
$\forall$ БА вводится *взвешенное число допущенных ошибок* - сумма весов всех объектов, на которых алгоритм ошибся:  $$E(g_k,w)=\sum^N_{i=1} w_i I(g_k(x^{(i)})\ne d^{(i)},\  \sum^N_{i=1} w_i=1$$
**Отступ** - минимизируемый показатель качества распознавания: $$Q(g,\alpha)=\sum^N_{i=1}[M(x^{(i)})<0]= \sum^N_{i=1}[d^{(i)}\sum^L_{k=1} \alpha_k g_k(x^{(i)})<0] \rightarrow min $$

1. Одинаковые начальные веса $w_i=1/N, i=\overline{1,N}$ ($\Leftrightarrow$ априорным вероятностям)
2. $\forall$ шага $k=1,2...L$
	- обучение БА $g_k(x)$ на $XD^N$, минимизирующего  $E_k=min\ E(g_k,w)$ %%(минимизация происходит за счет изменения порого при котором точка переходит в другой класс)%%
	- *если* $E_k=0$ то $G(x)=g_k(x)$ [п.3]
	- если $E_k\geq 1/2$ то [п.4] %%критическая ошибка%%
	- зафиксировать $\alpha_k=\frac {1}{2} ln(\frac {1-E_k}{E_k})$ 
	- пересчет весов  $w^{'}_i=w_i exp(-\alpha_k d^{(i)} g_k(x^{(i)})), i=\overline{1,N}$
	- нормировка весов $\sum^N_{i=1} w^{'}_i=1$
3. Зафиксировать композицию  $G(x)=sign[\sum^L_{k=1} \alpha_k g_k(x)]$
4. Конец

**При перевзвешивании усиливается роль ошибочно классифицированных объектов, учитываются ранее допущенные ошибки**

$\Leftrightarrow$
замена отступов на $[M(x^{(i)})<0]\leq exp[-M(x^{(i)})]$
минимизация функции$$\overline Q(g,X)=\sum^N_{i=1}exp[-M(x^{(i)})]\geq Q(g,X), H(M)=exp(-M)$$
В качестве БА чаще всего используются **деревья решений малой высоты** и простые правила порогового типа.
