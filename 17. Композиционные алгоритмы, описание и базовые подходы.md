
# Композиционные алгоритмы

основаны на объединении ансамбля из нескольких *простых* классификаторов, каждый из которых дает свое решение, а из совокупности этих решений формируется общий результат.

Пусть есть выборка $X$ - образы, $D$ - принадлежность к классу.

$$X^N = \lbrace x^{(1)}..x^{(N)}\rbrace, D^M = \lbrace d^{(1)}..d^{(M)}\rbrace$$

Вводится вспомогательное множество оценок $R$: для которых: $g'(x) = g'[g(x)],$  где $g: X \rightarrow R$ - базовый алгоритм (БА), $g': R \rightarrow D$ - решающее правило (например sign). 

Тогда композицией БА будет:

$$G'(x) = G'[G(g_1(x), g_2(x),..., g_L(x))], G:R^L \rightarrow R, G': R \rightarrow D$$

$G$ называется корректирующей операцией. Введение такой операции позволяет делать всякие манипуляции с полученными от БА решениями, например присвоить вес или еще чего.


Базовый алгоритм (классификатор) двух классов называется:
1. **плохим**, если вероятность ошибки $p_g \ge 1/2$
2. **слабым**, если вероятность ошибки $p_g = 1/2 - \epsilon$
3. **сильным**, если вероятность ошибки $p_g = \epsilon$

Стандартный вариант линейного ансамбля базовых алгоритмов:

$$G(x) = G[\sum^L_{k=1}\alpha_kg_k(x)]$$

Решающее правило

$$G'(x) = sign[G(X)]$$

Весовые коэффициенты

$$\sum_{k=1}^L \alpha_k = 1$$
Если $\alpha$ одинаковы, то тогда такое объединение называется **простым голосованием**.

# Бустинг

boosting - улучшение

Главная идея бустинга в том, что у нас есть несколько уровней композиций, каждая следующая (итерация) пытается исправить недостатки предыдущей.

# Бэггинг

bagging - bootstrap aggregating - агрегированный бутстреп.

**Bootstrap** - подход, в котором мы случайно извлекаем нескольких подмножеств примеров из обучающей выборки для получения устойчивых оценок.

Главная идея бэггинга в том, что базовые алгоритмы будут наиболее незаисимы друг от друга (они будут обучаться на разных данных).

# Случайный лес на основе бэггинга (random forest)

Случайный лес - ансамбль, где базовыми классификаторами выступают [[16. Алгоритмы распознавания на основе деревьев решений|деревья решений]]. 

**Два типа инъеции случайности:**
1. Для каждого дерева выбирается случайная подвыборка. Подвыборки пересекаются. Обычно такая подвыборка содержит ~63% от исходной обучающей выборки.
2. Деревья строятся по разным признакам.

**Виды объединения решений:**
1. для классификации - голосование
2. для регрессии - суммирование

**Алгоритм построения:**
В цикле для $k = \overline{1,L}$
1. Сформировать бутстреп выборку $XD^N_k$
2. По этой выборке индуцировать не усеченное дерево решений $T_k$ с минимальным количеством наблюдений в терминальных вершинах $n_{min},$ рекурсивно следуя процедуре:
	1) из исходного набора $n$ признаков выделить $p$ признаков
	2) из $p$ выбрать признак, обеспечивающий наилучшее расщепление
	3) расщепить выборку для вершины надве подвыборки
В результате получается ансамбль деревьев: $\lbrace T_k, k = \overline{1,L} \rbrace$

**Как распознавать:**

Выбирается класс $x \in \omega_j$, который максимальное число раз выбран базовыми классификаторами:

$$\tilde d (x) = i = \arg \max _{j=\overline{1,L}} \left \lbrace \sum^L_{k=1} I_k(x \in \omega_j) \right \rbrace, \ \ I_k = \begin{cases} 1, x \in \omega_j, \\ 0, x \notin \omega_j \end{cases}$$

Оценку вероятности ошибочной классификации можно проводить по исходной обучающей выборке методом **Out-Of-Bag**

При **OOB** распознавание для каждого вектора $x^{(*)} \in X^N$  происходит только с использованием деревьев которые строились без использования $x^{(*)}$.

**Плюсы случайных лесов:**
1. повышение достоверности за счет двойной инъекции случайности
2. сложная задача усечения снимается
3. проблема переобучения не такая уж проблема, поскольку ансамбль ДР уже не является единственным детально настроенным алгоритмом
4. простота настройки (параметры: количество деревьев и количество признаков для расщепления)