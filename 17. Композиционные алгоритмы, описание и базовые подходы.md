
**Композиционные алгоритмы** основаны на объединении ансамбля из нескольких *простых* классификаторов, каждый из которых дает свое решение, а из совокупности этих решений формируется общий результат.

Пусть есть выборка $X$ - образы, $D$ - принадлежность к классу.

$$X^N = \lbrace x^{(1)}..x^{(N)}\rbrace, D^M = \lbrace d^{(1)}..d^{(M)}\rbrace$$

Вводится вспомогательное множество оценок $R$: для которых: $g'(x) = g'[g(x)],$  где $g: X \rightarrow R$ - базовый алгоритм (БА), $g': R \rightarrow D$ - решающее правило (например sign). 

Тогда композицией БА будет:

$$G'(x) = G'[G(g_1(x), g_2(x),..., g_L(x))], G:R^L \rightarrow R, G': R \rightarrow D$$

$G$ называется корректирующей операцией. Введение такой операции позволяет делать всякие манипуляции с полученными от БА решениями, например присвоить вес или еще чего.


Базовый алгоритм (классификатор) двух классов называется:
1. **плохим**, если вероятность ошибки $p_g \ge 1/2$
2. **слабым**, если вероятность ошибки $p_g = 1/2 - \epsilon$
3. **сильным**, если вероятность ошибки $p_g = \epsilon$

Стандартный вариант линейного ансамбля базовых алгоритмов:

$$G(x) = G[\sum^L_{k=1}\alpha_kg_k(x)]$$

Решающее правило

$$G'(x) = sign[G(X)]$$

Весовые коэффициенты

$$\sum_{k=1}^L \alpha_k = 1$$
Если $\alpha$ одинаковы, то тогда такое объединение называется **простым голосованием**.

# Бустинг

boosting - улучшение

Главная идея бустинга в том, что у нас есть несколько уровней композиций, каждая следующая (итерация) пытается исправить недостатки предыдущей.

# Бэггинг

bagging - bootstrap aggregating - агрегированный бутстреп.

**Bootstrap** - подход, в котором мы случайно извлекаем нескольких подмножеств примеров из обучающей выборки для получения устойчивых оценок.

Главная идея бэггинга в том, что базовые алгоритмы будут наиболее незаисимы друг от друга (они будут обучаться на разных данных).

# Случайный лес на основе бэггинга (random forest)

Случайный лес - ансамбль, где базовыми классификаторами выступают [[16. Алгоритмы распознавания на основе деревьев решений|деревья решений]]. 

**Два типа инъеции случайности:**
1. Для каждого дерева выбирается случайная подвыборка. Подвыборки пересекаются. Обычно такая подвыборка содержит ~63% от исходной обучающей выборки.
2. Деревья строятся по разным признакам.

**Виды объединения решений:**
1. для классификации - голосование
2. для регрессии - суммирование

**Алгоритм построения:**
В цикле для $k = \overline{1,L}$
1. Сформировать бутстреп выборку $XD^N_k$
2. По этой выборке индуцировать не усеченное дерево решений $T_k$ с минимальным количеством наблюдений в терминальных вершинах $n_{min},$ рекурсивно следуя процедуре:
	1) из исходного набора $n$ признаков выделить $p$ признаков
	2) из $p$ выбрать признак, обеспечивающий наилучшее расщепление
	3) расщепить выборку для вершины надве подвыборки
В результате получается ансамбль деревьев: $\lbrace T_k, k = \overline{1,L} \rbrace$

**Как распознавать:**

Выбирается класс $x \in \omega_j$, который максимальное число раз выбран базовыми классификаторами:

$$\tilde d (x) = i = \arg \max _{j=\overline{1,L}} \left \lbrace \sum^L_{k=1} I_k(x \in \omega_j) \right \rbrace, \ \ I_k = \begin{cases} 1, x \in \omega_j, \\ 0, x \notin \omega_j \end{cases}$$

Оценку вероятности ошибочной классификации можно проводить по исходной обучающей выборке методом **Out-Of-Bag**

При **OOB** распознавание для каждого вектора $x^{(*)} \in X^N$  происходит только с использованием деревьев которые строились без использования $x^{(*)}$.

**Плюсы случайных лесов:**
1. повышение достоверности за счет двойной инъекции случайности
2. сложная задача усечения снимается
3. проблема переобучения не такая уж проблема, поскольку ансамбль ДР уже не является единственным детально настроенным алгоритмом
4. простота настройки (параметры: количество деревьев и количество признаков для расщепления)

#  Композиции на основе бустинга

Особенности
1. итеративность процесса обучения
2. учет предыдущих результатов классификации образов из выборки при выполнении каждой итерации
3. учет работы ранее используемых БА (допущенные ошибки)
Цель: усилить слабые классификаторы.
### AdaBoost (adaptive boosting)

Пусть есть выборка $XD^N$ для двух классов $\omega_1, \omega_2, d^{(i)}\in \lbrace -1, +1 \rbrace$.
При формировании композиции выполняется $L$ итераций.
БА - "пни" или деревья небольшой глубины $g_k(x) =\begin{cases} +1 \rightarrow x \in \omega_1 \\ -1 \rightarrow x \in \omega_2 \end{cases}$.
Итоговый алгоритм:

$$G(x) = sign\left[\sum^L_{k=1} \alpha_k g_k(x) \right]$$

Для каждого БА вводится взвешенное число допущенных ошибок:

$$E(g_k, w) = \sum^N_{i=1} w_i I(g_k(x^{(i)}) \ne d^{(i)}), \sum^N_{i=1} w_i =1$$

Вес пня $\alpha_k$ расчитывается как

$$\alpha_k = \frac{1}{2}\ln\left(\frac{1-E_k}{E_k}\right)$$

Общий функционал качества распознавания - **отступ** - который надо минимизировать:

$$Q(g, \alpha) = \sum^N_{i=1} \left[M(x^{(i)})<0\right] = \sum^N_{i=1}\left[d^{(i)} \sum_{k=1}^L \alpha_k g_k(x^{(i)}) <0 \right] \rightarrow \min$$

**Алгоритм:**

1. Назначить начальные веса $w_i = \frac{1}{N}$ - (имеет смысл априорных вероятностей);
2. Для каждого шага $k=1,2,..L$;
	1) Обучить БА $g_k(x)$ на $XD^N$ который минимизирует взвешенную ошибку классификации $E_k = \min E(g,w)$;
	2) Если $E_K = 0$, то $G(x) = g_k(x)$, переход к п.3;
	3) Если $E_k > 1/2$, то переход к п.4;
	4) Вычислить коэффициент $\alpha_k$ по формуле выше и зафиксировать его как вес БА;
	5) Пересчитать веса обучающих примеров: $w'_i = w_i \exp (-\alpha_k d^{(i)}g_k(x^{(i)}))$;
	6) Выполнить нормировку коэффициентов $\sum^N_{i=1} w'_k=1$;
3. $G_k = sign\left[ \sum^L_{k=1} \alpha_k g_k(x)\right]$ - итоговая композиция;
4. Окончание.

При взвешивании усиливается роль тех наблюдений, на которых допущены ошибки -> так учитываются ошибки с предыдущих шагов.