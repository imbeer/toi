
Если данные нельзя линейно разделить с маленькой ошибкой, то нужно **преобразовать образов обучающих выборок в образы другого пространства большей размерности**

$$\phi: R^n \rightarrow R^m, n \le m$$

После выполнения преобразования мы можем надеяться, что образы будут линейно разделимы. 

# Теорема Ковера

нелинейное преобразование сложной задачи классификации образов в пространство более высокой размерности повышает вероятность линейной разделимости образов.

# Пример преобразования

**Примеры линейно неразделимых образов:**
- Исключающее ИЛИ (класс 0 и класс 1 пересекают друг друга и линейно неразделимы)
- "Заплетенные восьмерки" - смесь из гауссовых случайных величин, "заплетенностью" которой $dm$ можно манипулировать.

Для построения преобразования можно использовать РБФ (радиально базисные функции). С удалением от центра $c$ функция стремится к нулю.

$$ \phi_j(x) = \phi(||x - c^{(j)}||) = \exp\left(- \frac{||x - c^{(j)}||^2}{2 \sigma_j^2}\right),\ \ j = \overline{1, N} $$
В задаче "исключающего или" центры функций нужно расположить в точках исходных образов: $(0,0) (1,1) (1,0) (0,1)$ 

Тогда получается преобразование $R^2 \rightarrow R^4: (\phi_1(x), \phi_2(x), \phi_3(x), \phi_4(x))$ 

$\sigma$ влияет на то насколько жирными получаются эти купола от РБФ (ну как дисперсия типо). Его нужно выбрать таким, чтобы области РБФ (где > 0) не пересекались.
Тогда пусть:

$$||x - c^{(j)}|| > 0.25: \phi_j(x) < \epsilon, \epsilon > 0, \epsilon \rightarrow 0$$

Тогда можно задать веса $w$ и построить линейно разделяющую функцию

$$\begin{align}
w_1 = \phi_1(x^{(1,1)}) = 1, \\ w_2 = \phi_2(x^{(1,2)}) = 1, \\
w_3 = -\phi_3(x^{(2,1)}) = -1, \\ w_4 = -\phi_4(x^{(2,2)}) = -1
\end{align}$$

Тогда если подставить значения первого или второго класса в эту функцию:

$$g(y) = y^Tw = \sum_{j=1}^4 y_j w_j$$

то она гарантированно примет значения:

$$w_1: g(y) \ge 1 - 2\epsilon >0, w_2: g(y) \le -1 + 2\epsilon < 0$$

Тогда образы получаются линейно разделимыми -> мы получили *спрямляющее пространство*. 

Правило решения после перехода в спрямляющее пространство имеет общий вид:

$$\alpha = sign[y^Tw+w_0] = sign[\phi(x)^Tw + w_0]$$

Спрямляющее пространство должно быть наделено скалярным произведением, потому что если в исходном алгоритме было скалярное произведение $x^Tz$ то в спрямляющем пространстве используется $\phi(x)^T\phi(z)$. 

# Трюк с ядром

Вместо того чтобы преобразовывать в другое пространство мы будем заменять прямое скалярное произведение на нелинейную функцию **"ядро скалярного произведения"**, в котором скалярные произведения были бы в неявном виде. 

Ядро скалярного произведения:

$$K(x,z) = \phi(x)^T \phi(z),$$
где $\phi$ - отображение в спрямляющее пространство.

 Пример - расстояние между векторами $y = \phi(x), y'=\phi(z)$:

$$\begin{gathered} 
d(y,y') = [(y - y')^T(y-y')]^{1/2} = [\phi(x)^T\phi(x) - 2 \phi(x)^T\phi(z) + \phi(z)^T\phi(z)]]^{1/2} = \\ = [K(x,x) - 2K(x,z) + K(z,z)]^{1/2}
\end{gathered}$$

Короче мы ищем не скалярное произведение в новом пространстве, мы ищем преобразование скалярного произведения в исходном. Если такие преобразования не используют явно переход в спрямляющее пространство, то это может еще и буст к производительности дать.

# Теорема Мерсера

Функция $K(x,z)$ является ядром тогда и только тогда, когда она симметрична $K(x,z) = K(z,x)$ и неотрицательно определена, то есть для любой конечной выборки $X^N = {x^{(1)}...x^{(N)}}$ матрица $||K(x^{(i), x^{(j)}})||$ неотрицательно определена.

# Примеры ядер

$$K(x,z) = \exp \left(- \frac{||x - z||^2}{2 \sigma ^2}\right)$$

$$K(x,z) = \left( x^Tz + 1 \right)$$

$$K(x,z) = \tanh \left( ax^Tz + b \right)$$
