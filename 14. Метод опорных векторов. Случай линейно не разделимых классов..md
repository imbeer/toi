
## Общая постановка задачи

На практике обучающие выборки часто не являются линейно разделимыми, то есть не существует гиперплоскости, которая бы безошибочно разделяла все образы двух классов. В этом случае метод опорных векторов обобщается за счёт допущения наличия ошибок классификации на обучающих данных.

Для этого ослабляются ограничения линейной разделимости и вводятся дополнительные переменные, характеризующие величину ошибки классификации.

## Введение переменных ошибок

Для каждого объекта обучающей выборки вводится неотрицательная переменная
$$ \xi_i \ge 0, \quad i = 1,\dots,N, $$
характеризующая величину ошибки на объекте $x^{(i)}$.

Возможны следующие случаи:
- $\xi_i = 0$ — объект классифицирован правильно и лежит вне разделяющей полосы;
- $0 < \xi_i < 1$ — объект лежит внутри разделяющей полосы, но классифицирован правильно;
- $\xi_i \ge 1$ — объект классифицирован ошибочно.

## Оптимизационная задача с ошибками

С учётом введённых переменных задача оптимизации принимает вид:
$$
\begin{cases}
\displaystyle \min \; \frac{1}{2} w^T w + C \sum_{i=1}^N \xi_i \\
d^{(i)} (x^{(i)T} w - b_0) \ge 1 - \xi_i, \quad i = 1,\dots,N \\
\xi_i \ge 0, \quad i = 1,\dots,N
\end{cases}
$$

Здесь параметр $C$ — управляющий параметр метода, определяющий компромисс между максимизацией ширины разделяющей полосы и минимизацией суммарной ошибки.

## Функция Лагранжа и условия оптимальности

Для решения задачи вводятся множители Лагранжа $\lambda_i \ge 0$ и $\eta_i \ge 0$.

Функция Лагранжа имеет вид:
$$
L(w, b_0, \xi, \lambda, \eta) =
\frac{1}{2} w^T w
- \sum_{i=1}^N \lambda_i \left[ d^{(i)} (x^{(i)T} w - b_0) - 1 + \xi_i \right]
- \sum_{i=1}^N \eta_i \xi_i
+ C \sum_{i=1}^N \xi_i
$$

Необходимыми условиями седловой точки являются равенства нулю производных:
$$ \frac{\partial L}{\partial w} = 0, \quad \frac{\partial L}{\partial b_0} = 0, \quad \frac{\partial L}{\partial \xi_i} = 0 $$

Отсюда следует:
$$ w = \sum_{i=1}^N \lambda_i d^{(i)} x^{(i)} $$
$$ \sum_{i=1}^N \lambda_i d^{(i)} = 0 $$
$$ \lambda_i \le C $$

## Типы опорных векторов

В случае линейно не разделимых классов возможны три типа объектов обучающей выборки:

1. $\lambda_i = 0$ — объект классифицирован правильно и не влияет на решение (периферийный вектор);
2. $0 < \lambda_i < C$ — объект лежит на границе разделяющей полосы (граничный опорный вектор);
3. $\lambda_i = C$ — объект лежит внутри разделяющей полосы или классифицирован ошибочно (опорный нарушитель).

Таким образом, в формировании решения участвуют не только граничные опорные векторы, но и нарушители.

## Вычисление смещения

Для любого опорного вектора с $\lambda_i > 0$ величина смещения определяется из соотношения:
$$ b_0 = x^{(i)T} w - d^{(i)} $$

На практике для повышения устойчивости решения используется медиана:
$$
b_0 = \operatorname{med}\{x^{(i)T} w - d^{(i)}\}, \quad \lambda_i > 0
$$

## Итоговый классификатор

Окончательное решающее правило имеет вид:
$$
g'(x) = \text{sign}\left( \sum_{i=1}^N \lambda_i d^{(i)} (x^{(i)T} x) - b_0 \right)
$$

В отличие от случая линейной разделимости, в классификации участвуют как граничные опорные векторы, так и опорные нарушители, что может снижать устойчивость алгоритма при наличии шумовых выбросов.

## Нелинейная разделимость и kernel trick

В случае принципиально линейно не разделимых классов применяется приём перехода в спрямляющее пространство признаков с помощью отображения $\varphi(x)$.

Классификатор сохраняет структуру:
$$
g'(x) = \text{sign}\left( \sum_{i=1}^N \lambda_i d^{(i)} \langle \varphi(x^{(i)}), \varphi(x) \rangle - b_0 \right)
$$

Вводя ядро скалярного произведения
$$ K(x^{(i)}, x) = \langle \varphi(x^{(i)}), \varphi(x) \rangle, $$
получаем окончательный вид классификатора:
$$
g'(x) = \text{sign}\left( \sum_{i=1}^N \lambda_i d^{(i)} K(x^{(i)}, x) - b_0 \right)
$$

Использование функций ядра позволяет реализовать нелинейные разделяющие поверхности без явного задания отображения $\varphi(x)$.
