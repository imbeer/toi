## 1. Непараметрическое обучение в задачах распознавания
Непараметрическое обучение предполагает использование непараметрического оценивания в процессе обучения.
Непараметрическое оценивание направлено на оценивание неизвестной **плотности распределения** $p(x)$ в целом. В отличие от параметрического оценивания нам не только неизвестны параметры распределения, но и сам закон распределения.
Чтобы оценка $\tilde{p}(x)$ была максимально близка к истинному распределению она должна обладать следующими свойствами:
- **Состоятельность:** сходимость по вероятности к истинному значению при росте выборки.
    $$\lim_{N\rightarrow\infty} P[||\tilde\theta(X^N)-\theta|| < \epsilon], \epsilon > 0 $$
- **Асимптотическая несмещенность:** математическое ожидание оценки должно стремиться к истинной плотности.
  $$\lim_{N\rightarrow\infty} M[\tilde\theta(X^N)] = M[\theta]$$
## 2. Метод Парзена(Метод ядерных оценок)
Суть метода заключается в том, что плотность в точке $x$ оценивается по количеству наблюдений из обучающей выборки, попавших в некоторую окрестность этой точки.
Формула для одномерного случая:
$$\tilde p(x) = \frac{1}{Nh}\sum_{i=1}^N\phi \left(\frac{x-x^{(i)}}{h} \right)$$
Здесь:
- $N$ - объём обучающей выборки.
- $h$ - коэффициент сглаживания (ширина окна)
- $\phi(\cdot)$ - ядро (оконная функция)
## 3. Роль ядра и коэффициента $h$
Ядро $\phi$ определяет форму вклада каждой точки выборки в итоговую оценку. Самый простой вариант - прямоугольное окно, которое просто считает точки внутри интервала. Прямоугольное ядро:
$$\phi(u/h) = \begin{cases} 1, & |u/h| \le 1/2 \\ 0, & |u/h| > 1/2 \end{cases}$$
Также используют и другие функции в качестве ядра:
- Треугольная
  $$\frac{1}{h}\phi(\frac{u}{h}) = \begin{cases} \frac{1}{h}(1-|u/h|), & |u/h| \le 1 \\ 0, & |u/h| > 1 \end{cases}$$
- Гауссовская
  $$\frac{1}{h}\phi(\frac{u}{h}) = \frac{1}{\sqrt{2\pi h}}\exp \left(-\frac{u^2}{2h^2}\right)$$
- Показательная
  $$\frac{1}{h}\phi(\frac{u}{h}) = \frac{1}{2 h}\exp \left(-\left|\frac{u}{h}\right|\right)$$
Ширина окна $h$ влияет на ширину промежутка к которому мы применяем функцию. При слишком малом $h$ оценка слишком сильно подстраивается под конкретные точки обучающей выборки (переобучается). При слишком большом $h$ оценка становится слишком грубой и не отражает реальных особенностей распределения.
## 4. Решающее правило
Поскольку метод Парзена  даёт нам оценку плотности $\tilde p(x|\omega_i)$ для каждого класса $\omega_i$, мы можем использовать подстановочный алгоритм. Мы заменяем истинные плотности в формуле Байеса на их непараметрические оценки.
Для двух классов правило будет выглядеть так:
$$x \in \omega_1, \text{если }\tilde p(x|\omega_1)P(\omega_1) > \tilde p(x|\omega_2,)P(\omega_2)$$