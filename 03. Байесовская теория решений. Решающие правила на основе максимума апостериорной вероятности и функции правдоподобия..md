
## Статистический подход

- Образы объектов в пространстве используемых признаков представляются как реализации случайного вектора $x = (x_1, \dots, x_n)^T$.
- Каждое значение этого вектора  $x = (x_1, \dots, x_n)^T \in \mathbb{R}^n$ — образ конкретного объекта.
- Любой образ может принадлежать одному из M классов объектов, в совокупности со составляющих конечное множество альтернативных статистических гипотез: $\Omega = \{\omega_i, i = \overline{1,M}\}$.
- Kоличество классов и их исходные наименования известны
- Априорные вероятности появления объектов различных классов – априорные вероятности гипотез $\omega_i   :Pr(\omega = \omega_i) = p(\omega_i), i = \overline{1,M}$.
- Cтатистически описания век тора признаков каждого класса x в виде условных плотностей распределения вероятностей: $f_x(x/\omega_i) = p(x/\omega_i), i = \overline{1,M}$ — функции правдоподобия классов.

По формуле Байеса апостериорные вероятности:

$$p(\omega_i / x) = \frac{p(x / \omega_i) p(\omega_i)}{p(x)}, \quad i = \overline{1,M}, \quad p(x) = \sum_{i=1}^{M} p(x / \omega_i) p(\omega_i)= \sum_{i=1}^{M} p(x , \omega_i)$$

## Задача: 
для каждого $x$ принять решение $\alpha(x) \in A = \{\alpha_i, i = \overline{1,M}\}$, разбивая пространство на области $\Gamma_i$, где $\alpha_i = \alpha(x) : x \in \Gamma_i \rightarrow \omega_i$.

## Синтез решающих правил

### Критерий максимума апостериорной вероятности (МАВ)

Важный частный случай МУР — использование симметричных штрафных функций с нулевой платой за правильное решение:

$$
\lambda_{ij} = \lambda(\alpha_i / \omega_j) = \begin{cases} 0, & i = j \\ 1, & i \neq j \end{cases} \quad i = \overline{ 1, M}, \quad j = \overline{ 1, M}.
$$

Это означает, что влияние всех ошибочных решений одинаково, а плата за правильное решение — нулю.

Условный риск преобразуется:

$$
r(\alpha_i / x) = \sum_{j=1, j≠i}^{M} p(\omega_j / x) = 1 - p(\omega_i / x), \quad i = \overline{1, M}.
$$

Принятие решения эквивалентно максимуму апостериорной вероятности гипотезы.

Байесовское решающее правило для многих классов — система неравенств для апостериорных вероятностей:

$$
\omega_i : p(\omega_i / x) \ge p(\omega_j / x), \quad j = \overline{1, M}, \quad i \ne j \qquad (4a)
$$

Или для отношений правдоподобия:

$$\omega_i : l_{ij}(x) = \frac{p(x / \omega_i)}{p(x / \omega_j)} 
\underset{\omega_2}{\overset{\omega_1}{\gtrless}} l_{ij}=
\frac{p(\omega_j)}{p(\omega_i)}, \quad j = \overline{1, M}, \quad i \ne j. \qquad (4b)$$

Для двух классов:

$$l(x) = \frac{p(x / \omega_1)}{p(x / \omega_2)} 
\underset{\omega_2}{\overset{\omega_1}{\gtrless}} 
\frac{p(\omega_2)}{p(\omega_1)} = l_0. (5)$$


Особенность МАВ: отказ от обоснования штрафных функций, порог зависит только от априорных вероятностей, требуется меньше информации по сравнению с МУР.

### Критерий максимального правдоподобия (МП)
Еще один частный случай: в дополнение к отсутствию обоснованных штрафных функций, априорные вероятности одинаковы $p(\omega_i) = 1/M$, $i = \overline{1, M}$ или неизвестны.
Решающие правила (4) преобразуются:

$$\omega_i: p(x/\omega_i) \ge p(x/\omega_j), \quad j = \overline{1, M}, \quad i \ne j, \qquad (6a)$$

Или:

$$\omega_i: l_{ij}(x) = \frac{p(x/\omega_i)}{p(x/\omega_j)} 
\underset{\omega_2}{\overset{\omega_1}{\gtrless}} l_{ij}\equiv
1, \quad j = \overline{1, M}, \quad i \ne j. \qquad (6b)$$

Для двух классов:

$$l(x) = \frac{p(x / \omega_1)}{p(x / \omega_2)} 
\underset{\omega_2}{\overset{\omega_1}{\gtrless}} 
l_0 \equiv 1$$


Алгоритмы МП реализуют минимальный объем исходной информации, необходимой для синтеза алгоритма распознавания.
