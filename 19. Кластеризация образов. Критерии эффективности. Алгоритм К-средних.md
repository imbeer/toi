# Кластеризация образов.
> Разбиение множества объектов на кластеры по общим признакам
> Без учителя 

**Кластер** - группа однотипных объектов, расположенных недалеко друг от друга в пространстве признаков.

*Задача кластеризации:*
Смешанная обучающая выборка образов $X^N = \{ x^{(1)},...,x^{(N)} \}, \ x^*\in R^n,$ принадлежит **M**(может быть известно и не известно),
Заданы функции расстояния $d( x^{(i)},x^{(j)})$(степень сходства образов)

*Задача:*
В соответствии с критерием оптимальности выполнить разбиение смешанной выборки образов $Q_M=\{X^{N_1},...,X^{N_M}\}$ на группы $X^{N_i} = \{ x^{(1,i)},...,x^{(N_i,i)} \},i=\overline{1,M}$, со свойствами: $X^{N_i} \neq \emptyset, i=\overline{1,M}$;    $\bigcup_{i=1}^{M} X^{N_i} = X^N$;    $\bigcap_{i=1}^{M} X^{N_i} = \emptyset$

При неизвестном М единый критерий оптимальности отсутствует и задача не корректна, но может быть решена

## Критерии эффективности

1. Среднее суммы квадратов расстояния образов каждого кластера до его центра –***внутриклассовый разброс (ВР)***
   
   $$E_W ( X^N, Q_M ) = \sum_{i=1}^{M} \sum_{k=1}^{N_i} d( x^{(k,i)}, m_i )^2, \quad m_i = \frac{1}{N_i} \sum_{k=1}^{N_i} x^{(k,i)}, \quad i = \overline{1, M}$$ 
   
   Для евклидова расстояния - через матрицы рассеяния классов
   
   $$E_W ( X^N, Q_M ) = \text{tr} S_W = \text{tr} \left( \sum_{i=1}^{M} S_i \right), \ S_i = \sum_{k=1}^{N_i} ( x^{(k,i)} - m_i )( x^{(k,i)} - m_i )^T$$  
   
   $E_W ( X^N, Q_M ) \to \min$ (группирование с минимальной дисперсией) для известного М, при неизвестном - монотонное уменьшение числа кластеров  с увеличением М => не имеет смысла
2. Средняя сумма квадратов расстояний между центрами кластеров относительно общего центра -  ***межклассовый разброс (MР)***
   
   $$E_B ( X^N, Q_M ) = \sum_{i=1}^{M} N_i d( m_i, m )^2, \quad m = \frac{1}{N} \sum_{k=1}^{N} x^{(k)} = \frac{1}{N} \sum_{i=1}^{M} N_i m_i$$ 
   
   Для евклидова расстояния - через матрицу рассеяния  между классами 
   
   $$E_B ( X^N, Q_M ) = \text{tr} S_B, \quad S_B = \sum_{i=1}^{M} N_i (m_i - m)( m_i - m )^T$$  
   
   $E_B ( X^N, Q_M ) \to \max$ 
3. **Общая матрица рассеяния**(не зависит от разбиения):$S_T = S_W + S_B = const$

# Алгоритм К-средних.

М известно(К=М),  $E_W ( X^N, Q_M ) \to \min$, работает итеративно

**Алгоритм:**
1. Задаются начальные центры кластеров $m_i,i=\overline{1,M}$(случайно или на максимальном расстоянии)
2. Разбиение $Q_M$ на кластеры, по минимальному расстоянию до центров $m_i$, расчет $E_W$
3. Перерасчет центров: $$m^{'}_i=\frac{1}{N_i}\sum_{k=1}^{N_i} x^{(k,i)}, \quad i = \overline{1, M}$$ перерасчет $E^{'}_W$ 
4. Если $\delta_{\max} = \max_{i} | m_i - m'_i | > \delta_0$ : $m_i = m'_i, \quad i = \overline{1, M}$ переход на [п.2]
   Иначе - [остановка]
   $\delta_0$ - порог для фиксации остановки

При таком подходе  $E_W$ на соседних шагах не увеличиваются и алгоритм сходится за конечное число шагов - **направленный поиск решения**.
повысить устойчивость алгоритма можно неоднократным запуском со случайными начальными значениями и выбором того решения, которое покажет минимум  $E_W$


**Статистический подход(ЕМ)**
смешанная обучающая выборка образов $X^N = \{ x^{(1)},...,x^{(N)} \}, \ x^*\in R^n,$ принадлежит **M** порождающим классам, априорные вероятности $\pi_i=p(\omega_i)>0$  не известны доля класса в выборке, вид плотностей распределения задан $p(x/\theta_i)$ форма и расположение "облака"

*Задача:* найти неизвестные параметры $\theta = (\theta_1^T, \dots, \theta_M^T, \pi_1, \dots, \pi_M)^T$

Для поиска параметров используется **метод максимального правдоподобия**:$$p(X^N | \theta) = \prod_{k=1}^{N} \sum_{i=1}^{M} p(x^{(k)} | \theta_i) \pi_i  \to \max$$
Для решения этой задачи используется итерационный **EM-алгоритм**:
Алгоритм основан на введении вспомогательных **скрытых переменных** $q_{ik}$— апостериорных вероятностей принадлежности объекта $k$ к кластеру $i$.
1. **Инициализация:** Задаются случайные или эвристические начальные центры и веса.
2. **E-шаг (Ожидание):** Расчет вероятностей $q_{ik}$ для каждой точки на основе текущих параметров («мягкая» кластеризация).
3. **M-шаг (Максимизация):** Пересчет параметров кластеров (`π,m,Cπ,m,C`) так, чтобы они максимально точно описывали точки с учетом их новых вероятностей.
4. **Останов:** Повторение цикла до стабилизации значений (когда изменения станут меньше порога   $\delta_0$ ).