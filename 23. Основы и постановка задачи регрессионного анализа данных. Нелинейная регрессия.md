![[22. Основы и постановка задачи регрессионного анализа данных.  Уравнения линейной регрессии#Общие сведения]]

![[22. Основы и постановка задачи регрессионного анализа данных.  Уравнения линейной регрессии#Постановка задачи]]

# Нелинейная регрессия

Полученное для линейной регрессии решение (см. [[22. Основы и постановка задачи регрессионного анализа данных.  Уравнения линейной регрессии|вопрос 22]]) может использоваться при произвольном нелинейном преобразовании $x: \bar x = \psi(x)$, если неизвестные параметры входят в функционал линейным образом:

$$\psi(x,a) = a^T \psi(x), \quad \tilde y = \tilde a^T \psi(x).$$

Пусть отображение $\psi: \mathbb R^n \rightarrow \mathbb R^{s+1}$ осуществляет перевод входных переменных в новое пространство в общем случае другой размерности. Тогда $\psi(x) = \left(1, \psi_1(x), \ldots, \psi_s(x) \right)^T$. Вводя для всех элементов обучающей выборки замену $\bar x^{(i)} = \psi(x^{(i)}),\ i=\overline{1,N},\ \bar x \in \bar X \subseteq \mathbb R^s$, можно перейти к поиску решения задачи линейной регрессии для новых переменных

$$\tilde y = \psi(x, \tilde a) = \tilde a_0 + \sum_{k=1}^s \tilde a_k \bar x_k = \tilde a^T \bar x_e,$$

где $\tilde a$ ищется аналогичным образом.

## Полиномиальная регрессия

Представим полиномы $n$ переменных различной степени до $d$ включительно в виде линейной функции $s+1$ переменных, $s+1 = C_{n+d}^n$ (с добавлением фиктивной переменной $\bar x_0 = 1$):

$$\bar x_0 = 1,\ \bar x_i = x_i\ (i=\overline{1,N}),\ \bar x_{n+1} = x_1x_1,\ \bar x_{n+2} = x_1x_2,\ \ldots,\ \bar x_{n(n+1)-1} = x_nx_{n-1},\ \bar x_{n(n+1)} = x_nx_n,\ \ldots$$

При таком представлении каждое слагаемое искомой регрессии имеет вид

$$a_k\psi_k(x) = a_{i_1\ldots i_n}x_1^{i_1}\times x_2^{i_2} \times \ldots \times x_n^{i_n},\quad i_1+i_2+\ldots+i_n \leq d,\ k=\overline{1,s+1},$$

где $a_{i_1\ldots i_n}$ – исходные коэффициенты, которые приравниваются к коэффициентам линейной регрессии $a_k$ после замены переменных. В результате всегда можно представить исходный полином, описывающий линейную регрессию по отношению к $s$ новым входным переменным.

При такой замене элементы исходной обучающей выборки преобразуются в элементы обучающей выборки для новых переменных эквивалентным образом.

В частном случае, когда входная переменная является скалярной величиной, регрессия ищется изначально в виде

$$\psi(x,a) = a^T\psi(x) = \sum_{k=0}^d a_k x^k.$$

Такая регрессия называется **полиномиальной,** и её использование позволяет повысить точность представления данных в случаях, когда исходная модель является сугубо нелинейной.