В рамках статистического подхода предполагается, что:

- образы объектов в пространстве признаков представляются как реализации случайного вектора $$x = (x_1, \dots, x_n)^T \in \mathbb{R}^n$$

- любой образ может принадлежать одному из $M$ классов объектов, составляющих множество альтернативных статистических гипотез $$\Omega = \{\omega_i, \, i = 1,\dots,M\}$$
- количество классов и их наименования считаются известными

Для описания классов задаются:
- **априорные вероятности** гипотез $$p(\omega_i) = \Pr(\omega = \omega_i), \quad i = 1,\dots,M$$

- **функции правдоподобия** (условные плотности распределения) 
$$p(x / \omega_i) = f_x(x / \omega_i), \quad i = 1,\dots,M$$

По формуле Байеса:

$$p(\omega_i / x) = \frac{p(x / \omega_i) \, p(\omega_i)}{p(x)}, \quad i = 1,\dots,M$$

$$p(x) = \sum_{i=1}^M p(x / \omega_i) \, p(\omega_i) = \sum_{i=1}^M p(x, \omega_i)$$

**Решаемая задача состоит в том, чтобы для каждого образа** _x_ **выполнить действие – решение $\alpha(x)$, относящие его к тому или иному классу.**
$$\alpha(x) \in A = \{\alpha_i, \, i = 1,\dots,M\}$$

Разбиение пространства признаков на непересекающиеся области решений $\Gamma_i$: $$\Gamma_i \cap \Gamma_j = \emptyset, \quad i \neq j, \quad \bigcup_{i=1}^M \Gamma_i = \mathbb{R}^n$$ $$\alpha(x) = \alpha_i \quad \Leftrightarrow \quad x \in \Gamma_i$$

Обозначение: $$\alpha_i = \alpha(x) : \quad x \in \Gamma_i \quad \to \quad \omega_i$$

## Критерий минимума условного риска
**Цель** — минимизировать ожидаемые потери (риск) при принятии решения о классе объекта.

 Штрафные функции (потери) за решение $\alpha_i$ (выбор класса $\omega_i$), когда истинный класс $\omega_j$): 
 
 $$\lambda_{ij} = \lambda(\alpha_i / \omega_j), \quad i,j = 1,\dots,M$$

Функция **условного риска**:

$$r(\alpha_i / x) = \sum_{j=1}^M \lambda(\alpha_i / \omega_j) \, p(\omega_j / x), \quad i = 1,\dots,M$$

ФУР определяют суммарные потери за принятие решения $\alpha_i$ при получении данных _x_.
**Общий ожидаемый риск**:

$$R = \int r(\alpha(x) / x) \, p(x) \, dx = \sum_{i=1}^M \int_{\Gamma_i} r(\alpha_i / x) \, p(x) \, dx = \sum_{i=1}^M \sum_{j=1}^M  \int_{\Gamma_i} \lambda(\alpha_i / \omega_j)p(\omega_j, x) \, dx$$

Минимизация $R$ достигается минимизацией условного риска для каждого $x$:

$$r(\alpha(x) / x) = \min \{ r(\alpha_1 / x), \dots, r(\alpha_M / x) \}$$

**Правило минимума условного риска (МУР)**:
Правило принятия решения в ходе распознавания: выбор в пользу гипотезы $\omega_i$ для полученного образа _x_ осуществляется, если
$$\omega_i : \quad r(\alpha_i / x) \le r(\alpha_j / x) \quad j = 1,\dots,M, i \neq j$$

### Случай двух классов

$$ r(\alpha_1 / x) = \lambda_{11} p(\omega_1 / x) + \lambda_{12} p(\omega_2 / x) $$

$$ r(\alpha_2 / x) = \lambda_{21} p(\omega_1 / x) + \lambda_{22} p(\omega_2 / x) $$

Решение в пользу $\omega_1$, если $r(\alpha_1 / x) \le r(\alpha_2 / x)$. 
Преобразования:

$$(\lambda_{21} - \lambda_{11}) p(\omega_1 / x) \underset{\omega_2}{\overset{\omega_1}{\gtrless}}-(\lambda_{22} - \lambda_{12}) p(\omega_2 / x)$$
$$\frac{(\lambda_{21} - \lambda_{11}) p(x / \omega_1) p(\omega_1)}{p(x)} \underset{\omega_2}{\overset{\omega_1}{\gtrless}}\frac{(\lambda_{12} - \lambda_{22}) p(x / \omega_2) p(\omega_2)}{p(x)}$$

Критерий по отношению правдоподобия (l(x)):


$$ l(x) = \frac{p(x / \omega_1)}{p(x / \omega_2)}\underset{\omega_2}{\overset{\omega_1}{\gtrless}}\frac{(\lambda_{12} - \lambda_{22}) p(\omega_2)}{(\lambda_{21} - \lambda_{11}) p(\omega_1)} =l_0  $$

(при $\lambda_{21} - \lambda_{11} > 0$ и $\lambda_{12} - \lambda_{22} > 0$)

Алгоритм принятия решения сводится к сравнению $l(x)$ с порогом $l_0$, где порог $l_0$ зависит от штрафных функций и априорных вероятностей гипотез, определяющих априорные сведения о появлении классов образов.
Области решений в пространстве признаков определяются в этом случае как

$$\alpha_1: x\in\Gamma_1 : \quad l(x) \geq l_0$$

$$\alpha_2: x\in\Gamma_2 : \quad l(x) < l_0$$

а разделяющая граница между ними – уравнением $l(x) = l_0$
